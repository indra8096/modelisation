# Test 26.02.25
# Enoncé : 
# Créer un HMM aléatoire :
# Générer un modèle de Markov Caché avec un nombre défini d’états cachés et un alphabet d’observations.
# Initialiser les probabilités d’émission et de transition de manière aléatoire.
# Effectuer l’estimation de séquences :

# Lire une séquence d’observations à partir d’un fichier FASTA.
# Calculer les probabilités des chemins possibles à travers les états cachés en utilisant les algorithmes avant (forward) et arrière (backward).
# Améliorer le modèle avec des mises à jour itératives :

# Implémenter la mise à jour des paramètres du modèle via l’algorithme Expectation-Maximization (EM).
# Mettre à jour les probabilités initiales des états (π), les probabilités d’émission (E), et les probabilités de transition (T).
# Interpréter les résultats :

# Visualiser la convergence du modèle en affichant les paramètres mis à jour après plusieurs itérations.
# Comparer les probabilités avant et après l’apprentissage.
# Contraintes
# Utiliser la normalisation des probabilités (normd) pour assurer la cohérence des distributions.
# Mettre en place les fonctions pour le calcul des probabilités de transition et d’émission en s’appuyant sur les algorithmes de Forward-Backward.
# Tester ton modèle avec différentes séquences et observer son adaptation aux données.

